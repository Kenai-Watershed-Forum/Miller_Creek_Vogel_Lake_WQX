---
title: "Vogel Lake Outlet Hydrolabs"
output: html_document
date: "2022-10-10"
---


# Hydrolab Data

To assess variation in water quality throughout time, two OTT Hydrolab MS5 multiparameter water quality sondes ("Hydrolabs") were deployed at the inlet of Miller Creek where it exits Vogel Lake in summer 2021 and summer 2022. The pair of Hydrolabs were placed in the center of the stream midway through the water column suspended from a buoy. Prior to deployment, all Hydrolabs were calibrated in accordance with the Department of Environmental Conservation (DEC)-approved Quality Assurance Project Plan: Kenai River Watershed Monitoring Program requirements ([KWF, 2019](https://paperpile.com/app/p/7703451b-460d-00b4-82a0-1086ea2554c3)). Post-retrieval, Hydrolabs underwent quality assurance checks for all parameters assessed in the field. Prior to departure, quality control measures were taken to ensure proper data collection including cleaning of Hydrolabs and ensuring the use of calibration solutions prior to expiration dates.

Hydrolab data were assessed in Fall 2022. Parameters included temperature, pH, dissolved oxygen, conductivity, turbidity, and temperature. Results were found to be of mixed reliability for characterizing water quality at this site. Due to the site's remote location and the low flow rate in Miller Creek, the schedule necessary to maintain the hydrolabs (probe cleaning, battery replacement, calibration) was not maintained on a schedule that would be standard practice at a road accessible site. As a result, hydrolab data at this site is not published in this report. To access hydrolab data from this site contact Kenai Watershed Forum at  hydrology@kenaiwatershed.org or call their office at (907) 262-5449.

## QA/QC Methods (for visualization)

1.  Plot original datasets by individual logger, year, and parameter. Excise visually erroneous data by manually identified date range.
2.  Average simultaneous values at each site and year when loggers were deployed side-by-side.
3.  Plot averaged values

```{r , include=FALSE}
# clear environment
rm(list=ls())
#require packages
library(googlesheets4)
library(tidyverse)
library(hms)
library(janitor)
library(lubridate)
library(DT)
library(readxl)
library(plotly)

# spatial packages
library(leaflet)
library(sp)
library(rgeos)
library(maps)

# specify local directory where files will be accessed from
dir <- "input/hydrolab_data/vogel_lake_outlet_compiled"
```

<br>

## Site Location

```{r, out.width = '75%', out.height='75%'}
# make dataframe of coordinates of Hydrolab sites
site <- c("Vogel Lake Outlet")
latitude <- 60.9901	
longitude <- -150.437513
coords <- data.frame(site,latitude,longitude)
# create map
leaflet(data = coords) %>%
  addTiles() %>%  # Add default OpenStreetMap map tiles
  #fitBounds(-150, 60.04,-149.0, 60.02) %>%
  setView(-150.437513, 60.9901, zoom = 13) %>%
  addMarkers(~longitude, ~latitude, popup = ~as.character(site), label = ~as.character(site))
```

### Data upload and preparation

Upload files from local directory and prepare data for visualization

```{r}
#Import all Vogel Lake Outlet
# run read-in function on files in vogel lake outlet directory
vl_data <-list.files(path = , "input/hydrolab_data/vogel_lake_outlet_compiled",
               pattern = "*.csv", 
               full.names = T) %>% 
    map_df(~read_csv(., col_types = cols(.default = "c"), skip = 13)) %>%
  filter(!is.na(`HH:MM:SS`)) %>%
  mutate(site = "vogel_lake_outlet") %>%
  remove_empty("cols")


# rename columns
colnames(vl_data) <- c("date","time","temp_C","pH","cond_uS_cm","turb_ntu","do_mgl","batt_pct","logger_id","site")

# Final data frame cleaning steps
# create DateTime column
all_data <- vl_data %>% # add other data frames here if need be
  mutate(date = mdy(as.character(date))) %>%
  mutate(datetime = as.POSIXct(paste(date,time),
                        format = "%Y-%m-%d %H:%M:%S", 
                        tz = "America/Anchorage")) %>%
  mutate(year = year(date),
         day = yday(date)) %>%
  # round all observations to nearest 0.25 hour interval
  transform(datetime = round_date(datetime, "minute")) %>%
  arrange(datetime) 

  
# clean up parameter names 
all_data <- all_data %>%
  # remove unneeded parameters
  select(-batt_pct)

# make character vector of all unique logger IDs
loggers <- unique(all_data$logger_id)

# prep for plotting
all_data <- all_data %>%
  select(-date,-time) %>%
  transform(datetime = as.POSIXct(datetime)) %>%
  gather(key = "parameter", value = "value",temp_C,pH,turb_ntu,do_mgl,cond_uS_cm,) %>%
  transform(value = as.numeric(value)) %>%
    filter(!is.na(value),
           !is.na(logger_id)) 
```

------------------------------------------------------------------------

### Remove Erroneous Logger Data

Plot data series individually by sonde, parameter, and year; and visually identify and remove erroneous data.

#### Temperature

```{r}
# temp
ggplotly(
  all_data %>%
  filter(parameter == "temp_C") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Original Temperature Data")
)
  
```

<br>

```{r, echo = F}

# Plot cleaned-up temperature data

# read in temp data excise table
excise_temp <- read_excel("input/hydrolab_data/KWF_Hydrolabs_ExciseData_Vogel.xlsx", sheet = "Temperature") %>%
  transform(logger_id = as.character(logger_id)) %>%
  filter(!is.na(site))

# create table of observations to be removed; subsetted from overall data frame
excise_temp <- all_data %>%
  filter(parameter == "temp_C") %>%
  inner_join(excise_temp, by = c("logger_id","year","parameter","site")) %>%
  filter(day >= day_start & day <= day_end)

# remove manually identified observations from overall dataset
all_data <- anti_join(all_data,excise_temp)

# plot cleaned-up temp data
# temp
ggplotly(
  all_data %>%
  filter(parameter == "temp_C") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Corrected Temperature Data")
)
```

------------------------------------------------------------------------

#### pH

```{r}
# pH
ggplotly(
  all_data %>%
  filter(parameter == "pH") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Original pH Data")
)
  
```


```{r, echo = F}

# Plot cleaned-up pH data

# read in temp data excise table
excise_temp <- read_excel("input/hydrolab_data/KWF_Hydrolabs_ExciseData_Vogel.xlsx", sheet = "pH") %>%
  transform(logger_id = as.character(logger_id)) %>%
  filter(!is.na(site))

# create table of observations to be removed; subsetted from overall data frame
excise_temp <- all_data %>%
  filter(parameter == "pH") %>%
  inner_join(excise_temp, by = c("logger_id","year","parameter","site")) %>%
  filter(day >= day_start & day <= day_end)

# remove manually identified observations from overall dataset
all_data <- anti_join(all_data,excise_temp)

# plot cleaned-up temp data
# temp
ggplotly(
  all_data %>%
  filter(parameter == "pH") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Corrected pH Data")
)
```

<br>

***

#### Conductivity

```{r}
# pH
ggplotly(
  all_data %>%
  filter(parameter == "cond_uS_cm") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Original Conductivity Data")
)
  
```


```{r, echo = F}

# Plot cleaned-up cond data

# read in temp data excise table
excise_temp <- read_excel("input/hydrolab_data/KWF_Hydrolabs_ExciseData_Vogel.xlsx", sheet = "Conductivity") %>%
  transform(logger_id = as.character(logger_id)) %>%
  filter(!is.na(site))

# create table of observations to be removed; subsetted from overall data frame
excise_temp <- all_data %>%
  filter(parameter == "cond_uS_cm") %>%
  inner_join(excise_temp, by = c("logger_id","year","parameter","site")) %>%
  filter(day >= day_start & day <= day_end)

# remove manually identified observations from overall dataset
all_data <- anti_join(all_data,excise_temp)

# plot cleaned-up cond data
# temp
ggplotly(
  all_data %>%
  filter(parameter == "cond_uS_cm") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Corrected Conductivity Data")
)
```

<br>

***

#### Dissolved Oxygen

```{r echo = F}
# DO
ggplotly(
  all_data %>%
  filter(parameter == "do_mgl") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Original Dissolved Oxygen Data")
)
  
```

```{r}
# read in DO data excise table
excise_temp <- read_excel("input/hydrolab_data/KWF_Hydrolabs_ExciseData_Vogel.xlsx", sheet = "DO") %>%
  transform(logger_id = as.character(logger_id)) %>%
  filter(!is.na(site))

# create table of observations to be removed; subsetted from overall data frame
excise_temp <- all_data %>%
  filter(parameter == "do_mgl") %>%
  inner_join(excise_temp, by = c("logger_id","year","parameter","site")) %>%
  filter(day >= day_start & day <= day_end)

# remove manually identified observations from overall dataset
all_data <- anti_join(all_data,excise_temp)

# plot cleaned-up  data
# temp
ggplotly(
  all_data %>%
  filter(parameter == "do_mgl") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Corrected Dissolved Oxygen Data")
)


```


<br>

***

#### Turbidity

```{r echo = F}
# turbidity
ggplotly(
  all_data %>%
  filter(parameter == "turb_ntu") %>%
  ggplot(aes(day,value, color = logger_id), fill = as.factor(site)) +
  geom_point(pch = 21) +
  facet_grid( logger_id ~ year, scales = "free_y") +
  ggtitle("Original Turbidity Data")
)
  
```

Note: visual inspection indicates that all turbidity data available may be not interpretable. It is not recommended that these data be applied towards turbidity characterization for this site.

```{r}
# Turbidity: visual inspection suggests that ALL turbidity data si erroneous in some form. No turbdity data should be interpreted or applied.


```

